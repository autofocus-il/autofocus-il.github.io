<!DOCTYPE html>
<html class="fontawesome-i2svg-active fontawesome-i2svg-complete"><head>
    <meta charset="utf-8">
    <meta name="description" content="AutoFocus-IL leverages vision-language models (VLMs) to automatically identify and track key objects in demonstrations, generating temporal saliency maps that highlight causal visual signals while suppressing distractors.">
    <meta name="keywords" content="AutoFocus-IL">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>AutoFocus-IL: VLM-based Saliency Maps for Data-Efficient Visual Imitation Learning without Extra Human Annotations</title>

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async="" src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
    <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

    <style type="text/css">svg:not(:root).svg-inline--fa{overflow:visible}.svg-inline--fa{display:inline-block;font-size:inherit;height:1em;overflow:visible;vertical-align:-.125em}.svg-inline--fa.fa-lg{vertical-align:-.225em}.svg-inline--fa.fa-w-1{width:.0625em}.svg-inline--fa.fa-w-2{width:.125em}.svg-inline--fa.fa-w-3{width:.1875em}.svg-inline--fa.fa-w-4{width:.25em}.svg-inline--fa.fa-w-5{width:.3125em}.svg-inline--fa.fa-w-6{width:.375em}.svg-inline--fa.fa-w-7{width:.4375em}.svg-inline--fa.fa-w-8{width:.5em}.svg-inline--fa.fa-w-9{width:.5625em}.svg-inline--fa.fa-w-10{width:.625em}.svg-inline--fa.fa-w-11{width:.6875em}.svg-inline--fa.fa-w-12{width:.75em}.svg-inline--fa.fa-w-13{width:.8125em}.svg-inline--fa.fa-w-14{width:.875em}.svg-inline--fa.fa-w-15{width:.9375em}.svg-inline--fa.fa-w-16{width:1em}.svg-inline--fa.fa-w-17{width:1.0625em}.svg-inline--fa.fa-w-18{width:1.125em}.svg-inline--fa.fa-w-19{width:1.1875em}.svg-inline--fa.fa-w-20{width:1.25em}.svg-inline--fa.fa-pull-left{margin-right:.3em;width:auto}.svg-inline--fa.fa-pull-right{margin-left:.3em;width:auto}.svg-inline--fa.fa-border{height:1.5em}.svg-inline--fa.fa-li{width:2em}.svg-inline--fa.fa-fw{width:1.25em}.fa-layers svg.svg-inline--fa{bottom:0;left:0;margin:auto;position:absolute;right:0;top:0}.fa-layers{display:inline-block;height:1em;position:relative;text-align:center;vertical-align:-.125em;width:1em}.fa-layers svg.svg-inline--fa{-webkit-transform-origin:center center;transform-origin:center center}.fa-layers-counter,.fa-layers-text{display:inline-block;position:absolute;text-align:center}.fa-layers-text{left:50%;top:50%;-webkit-transform:translate(-50%,-50%);transform:translate(-50%,-50%);-webkit-transform-origin:center center;transform-origin:center center}.fa-layers-counter{background-color:#ff253a;border-radius:1em;-webkit-box-sizing:border-box;box-sizing:border-box;color:#fff;height:1.5em;line-height:1;max-width:5em;min-width:1.5em;overflow:hidden;padding:.25em;right:0;text-overflow:ellipsis;top:0;-webkit-transform:scale(.25);transform:scale(.25);-webkit-transform-origin:top right;transform-origin:top right}.fa-layers-bottom-right{bottom:0;right:0;top:auto;-webkit-transform:scale(.25);transform:scale(.25);-webkit-transform-origin:bottom right;transform-origin:bottom right}.fa-layers-bottom-left{bottom:0;left:0;right:auto;top:auto;-webkit-transform:scale(.25);transform:scale(.25);-webkit-transform-origin:bottom left;transform-origin:bottom left}.fa-layers-top-right{right:0;top:0;-webkit-transform:scale(.25);transform:scale(.25);-webkit-transform-origin:top right;transform-origin:top right}.fa-layers-top-left{left:0;right:auto;top:0;-webkit-transform:scale(.25);transform:scale(.25);-webkit-transform-origin:top left;transform-origin:top left}.fa-lg{font-size:1.3333333333em;line-height:.75em;vertical-align:-.0667em}.fa-xs{font-size:.75em}.fa-sm{font-size:.875em}.fa-1x{font-size:1em}.fa-2x{font-size:2em}.fa-3x{font-size:3em}.fa-4x{font-size:4em}.fa-5x{font-size:5em}.fa-6x{font-size:6em}.fa-7x{font-size:7em}.fa-8x{font-size:8em}.fa-9x{font-size:9em}.fa-10x{font-size:10em}.fa-fw{text-align:center;width:1.25em}.fa-ul{list-style-type:none;margin-left:2.5em;padding-left:0}.fa-ul>li{position:relative}.fa-li{left:-2em;position:absolute;text-align:center;width:2em;line-height:inherit}.fa-border{border:solid .08em #eee;border-radius:.1em;padding:.2em .25em .15em}.fa-pull-left{float:left}.fa-pull-right{float:right}.fa.fa-pull-left,.fab.fa-pull-left,.fal.fa-pull-left,.far.fa-pull-left,.fas.fa-pull-left{margin-right:.3em}.fa.fa-pull-right,.fab.fa-pull-right,.fal.fa-pull-right,.far.fa-pull-right,.fas.fa-pull-right{margin-left:.3em}.fa-spin{-webkit-animation:fa-spin 2s infinite linear;animation:fa-spin 2s infinite linear}.fa-pulse{-webkit-animation:fa-spin 1s infinite steps(8);animation:fa-spin 1s infinite steps(8)}@-webkit-keyframes fa-spin{0%{-webkit-transform:rotate(0);transform:rotate(0)}100%{-webkit-transform:rotate(360deg);transform:rotate(360deg)}}@keyframes fa-spin{0%{-webkit-transform:rotate(0);transform:rotate(0)}100%{-webkit-transform:rotate(360deg);transform:rotate(360deg)}}.fa-rotate-90{-webkit-transform:rotate(90deg);transform:rotate(90deg)}.fa-rotate-180{-webkit-transform:rotate(180deg);transform:rotate(180deg)}.fa-rotate-270{-webkit-transform:rotate(270deg);transform:rotate(270deg)}.fa-flip-horizontal{-webkit-transform:scale(-1,1);transform:scale(-1,1)}.fa-flip-vertical{-webkit-transform:scale(1,-1);transform:scale(1,-1)}.fa-flip-both,.fa-flip-horizontal.fa-flip-vertical{-webkit-transform:scale(-1,-1);transform:scale(-1,-1)}:root .fa-flip-both,:root .fa-flip-horizontal,:root .fa-flip-vertical,:root .fa-rotate-180,:root .fa-rotate-270,:root .fa-rotate-90{-webkit-filter:none;filter:none}.fa-stack{display:inline-block;height:2em;position:relative;width:2.5em}.fa-stack-1x,.fa-stack-2x{bottom:0;left:0;margin:auto;position:absolute;right:0;top:0}.svg-inline--fa.fa-stack-1x{height:1em;width:1.25em}.svg-inline--fa.fa-stack-2x{height:2em;width:2.5em}.fa-inverse{color:#fff}.sr-only{border:0;clip:rect(0,0,0,0);height:1px;margin:-1px;overflow:hidden;padding:0;position:absolute;width:1px}.sr-only-focusable:active,.sr-only-focusable:focus{clip:auto;height:auto;margin:0;overflow:visible;position:static;width:auto}.svg-inline--fa .fa-primary{fill:var(--fa-primary-color,currentColor);opacity:1;opacity:var(--fa-primary-opacity,1)}.svg-inline--fa .fa-secondary{fill:var(--fa-secondary-color,currentColor);opacity:.4;opacity:var(--fa-secondary-opacity,.4)}.svg-inline--fa.fa-swap-opacity .fa-primary{opacity:.4;opacity:var(--fa-secondary-opacity,.4)}.svg-inline--fa.fa-swap-opacity .fa-secondary{opacity:1;opacity:var(--fa-primary-opacity,1)}.svg-inline--fa mask .fa-primary,.svg-inline--fa mask .fa-secondary{fill:#000}.fad.fa-inverse{color:#fff}</style><link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

    <link rel="stylesheet" href="./assets/css/bulma.min.css">
    <link rel="stylesheet" href="./assets/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="./assets/css/bulma-slider.min.css">
    <link rel="stylesheet" href="./assets/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="./assets/css/index.css">
    <!-- <link rel="icon" href="./static/images/favicon.svg"> -->

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script defer="" src="./assets/js/fontawesome.all.min.js"></script>
    <script src="./assets/js/bulma-carousel.min.js"></script>
    <script src="./assets/js/bulma-slider.min.js"></script>
    <script src="./assets/js/index.js"></script>
  </head>
  <body>

    

    <section class="hero">
      <div class="hero-body">
        <div class="container is-max-desktop">
          <div class="columns is-centered">
            <div class="column has-text-centered">
              <h1 class="title is-1 publication-title">AutoFocus-IL: VLM-based Saliency Maps for <br>
                Data-Efficient Visual Imitation Learning <br>
                without Extra Human Annotations</h1>
              <div class="is-size-5 publication-authors">
                <!-- TODO: fill this parts with the authors data and links after acceptance -->
                <span class="author-block">
                  <!-- <a href="link-to-the-author-homepage"> -->
                    Anonymous Authors
                  <!-- </a>, -->
                </span>
              </div>

              <div class="is-size-5 publication-authors">
                <span class="author-block">Anonymous Institution</span>
              </div>

              <!-- <div class="is-size-6 publication-authors">
                <span class="author-block">* Equal Contribution</span>
              </div> -->

              <div class="is-size-5 publication-authors">
                <span class="author-block">IEEE International Conference on Robotics and Automation (ICRA) 2026.</span>
              </div>
                  


              <div class="column has-text-centered">
                <div class="publication-links">
                  <!-- TODO: fill this parts with the correct data and links after acceptance -->
                  <!-- PDF Link. -->
                  <!-- <span class="link-block">
                    <a href="" class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <svg class="svg-inline--fa fa-file-pdf fa-w-12" aria-hidden="true" focusable="false" data-prefix="fas" data-icon="file-pdf" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 384 512" data-fa-i2svg=""><path fill="currentColor" d="M181.9 256.1c-5-16-4.9-46.9-2-46.9 8.4 0 7.6 36.9 2 46.9zm-1.7 47.2c-7.7 20.2-17.3 43.3-28.4 62.7 18.3-7 39-17.2 62.9-21.9-12.7-9.6-24.9-23.4-34.5-40.8zM86.1 428.1c0 .8 13.2-5.4 34.9-40.2-6.7 6.3-29.1 24.5-34.9 40.2zM248 160h136v328c0 13.3-10.7 24-24 24H24c-13.3 0-24-10.7-24-24V24C0 10.7 10.7 0 24 0h200v136c0 13.2 10.8 24 24 24zm-8 171.8c-20-12.2-33.3-29-42.7-53.8 4.5-18.5 11.6-46.6 6.2-64.2-4.7-29.4-42.4-26.5-47.8-6.8-5 18.3-.4 44.1 8.1 77-11.6 27.6-28.7 64.6-40.8 85.8-.1 0-.1.1-.2.1-27.1 13.9-73.6 44.5-54.5 68 5.6 6.9 16 10 21.5 10 17.9 0 35.7-18 61.1-61.8 25.8-8.5 54.1-19.1 79-23.2 21.7 11.8 47.1 19.5 64 19.5 29.2 0 31.2-32 19.7-43.4-13.9-13.6-54.3-9.7-73.6-7.2zM377 105L279 7c-4.5-4.5-10.6-7-17-7h-6v128h128v-6.1c0-6.3-2.5-12.4-7-16.9zm-74.1 255.3c4.1-2.7-2.5-11.9-42.8-9 37.1 15.8 42.8 9 42.8 9z"></path></svg>
                      </span>
                      <span>arXiv</span>
                    </a>
                  </span> -->
                  
                  <!-- <span class="link-block">
                    <a href="" class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="ai ai-arxiv"></i>
                      </span>
                      <span>arXiv</span>
                    </a>
                  </span> -->

                  <!-- Video Link. -->
                  <!-- <span class="link-block">
                <a href="https://www.youtube.com/"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span> -->

                  <!-- Code Link. -->

                  <!-- <span class="link-block">
                    <a href="https://github.com/autofocus-il" class="external-link button is-normal is-rounded is-dark" target="_blank">
                      <span class="icon">
                        <svg class="svg-inline--fa fa-github fa-w-16" aria-hidden="true" focusable="false" data-prefix="fab" data-icon="github" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512" data-fa-i2svg=""><path fill="currentColor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg><!-- <i class="fab fa-github"></i> Font Awesome fontawesome.com -->
                      <!-- </span>
                      <span>CARLA Code</span>
                    </a>
                  </span> --> 

                  <!-- Code Link. -->
                   
                  <!-- <span class="link-block">
                    <a href="https://github.com/autofocus-il" class="external-link button is-normal is-rounded is-dark" target="_blank">
                      <span class="icon">
                        <svg class="svg-inline--fa fa-github fa-w-16" aria-hidden="true" focusable="false" data-prefix="fab" data-icon="github" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512" data-fa-i2svg=""><path fill="currentColor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg><!-- <i class="fab fa-github"></i> Font Awesome fontawesome.com -->
                      <!-- </span>
                      <span>Real-World Robot Code</span>
                    </a>
                  </span> -->

                  <!-- Dataset Link. -->

                  <!-- <span class="link-block">
                  <a href="https://drive.google.com" class="external-link button is-normal is-rounded is-dark" target="_blank">
                  <span class="icon">
                    <svg class="svg-icon" style="width: 1em; height: 1em;vertical-align: middle;fill: currentColor;overflow: hidden;" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg"><path d="M512 892.68816472c-179.30561336 0-319.74612799-63.19470985-319.74612799-143.80778519V275.09119949c0-80.63902496 140.44175032-143.7818356 319.74612799-143.78183559s319.74612799 63.14404634 319.74612799 143.78183559v473.78918004c0 80.61183966-140.44175032 143.8090209-319.74612799 143.80902089z m0-730.94114625c-173.04434365 0-289.31588759 58.61275048-289.31588759 113.35035948v473.78918005c0 54.74008041 116.26907255 113.32193849 289.31588759 113.32193848s289.31588759-58.58309378 289.31588759-113.32193848V275.09737795c0-54.74008041-116.26907255-113.35035948-289.31588759-113.35035948z m0 568.49783536c-179.30561336 0-319.74612799-63.14404634-319.74612799-143.75341459 0-8.40273023 6.78891175-15.2435412 15.2151202-15.24354121 8.42867984 0 15.21635589 6.84081097 15.2163559 15.24354121 0 54.7388447 116.26907255 113.32193849 289.31588759 113.32193849s289.31588759-58.58309378 289.31588758-113.32193849c0-8.40273023 6.78520467-15.2435412 15.21635589-15.24354121 8.42497276 0 15.21512021 6.84081097 15.21512022 15.24354121 0.00370709 80.61183966-140.44051463 143.75341459-319.7448923 143.75341459z m0-162.44701798c-179.30561336 0-319.74612799-63.14281064-319.74612799-143.75341459 0-8.40273023 6.78891175-15.24230551 15.2151202-15.24230551 8.42867984 0 15.21635589 6.83957527 15.2163559 15.24230551 0 54.7388447 116.26907255 113.32317419 289.31588759 113.32317419s289.31588759-58.58432949 289.31588758-113.32317419c0-8.40273023 6.78520467-15.24230551 15.21635589-15.24230551 8.42497276 0 15.21512021 6.83957527 15.21512022 15.24230551 0.00370709 80.61183966-140.44051463 143.75341459-319.7448923 143.75341459z m0-162.4408395c-179.30561336 0-319.74612799-63.14404634-319.74612799-143.7830713 0-8.42744415 6.78891175-15.21512021 15.2151202-15.21512019a15.18422782 15.18422782 0 0 1 15.2163559 15.21512019c0 54.74008041 116.26907255 113.35035948 289.31588759 113.3503595s289.31588759-58.61275048 289.31588758-113.3503595c0-8.42744415 6.78520467-15.21512021 15.21635589-15.21512019a15.18669921 15.18669921 0 0 1 15.21512022 15.21512019c0.00370709 80.64026065-140.44051463 143.78307129-319.7448923 143.7830713z"  /></svg>
                  </span>
                  <span>Real-World Robot Dataset</span>
                  </a>
                  </span>                   -->


                </div>

              </div>
            </div>
          </div>
        </div>
      </div>
    </section>


    <section class="hero teaser" style="margin-bottom: -20px;">
      <div class="hero-body">
        <div class="container is-max-desktop" style="text-align: center;">
          <div class="columns is-centered has-text-centered">
            <div class="column is-four-fifths">    
            <p style="text-align: center; font-size: 24px; margin-bottom: 0px;">
              <strong><em>Can VLMs' generated saliency maps improve imitation learning?</em></strong>
            </br> </br>
            </p>
              
          <div style="text-align: center;">
            <img src="./assets/files/Intro_fig.png" alt="intro fig" style="width: 80%;">
          </div>
          <br>
          An overview of three different approaches: While traditional imitation learning 
          suffers from causal confusion, gaze-based IL solves this by utilizing an expensive 
          solution of collecting human eye gaze data. However, <b>AutoFocus-IL</b> resolves this 
          issue by getting a saliency map annotated by a VLM to retain the benefits of 
          gaze-based IL without incurring the extra data collection costs.
          <hr>
        </div>
        
        </div>
        </div>
      </div>
    </section>

    <!-- Abstract. -->
    <section class="section">
      <div class="container is-max-desktop">
        <div class="columns is-centered has-text-centered">
          <div class="column is-four-fifths">
            
            <h2 class="title is-3">Abstract</h2>
            <div class="content has-text-justified">


              <p>
                We present <span style="color: maroon; font-weight: bold">AutoFocus-IL</span>, a simple yet effective method to improve data efficiency and generalization 
                in visual imitation learning by guiding policies to attend to task-relevant features rather than 
                distractors and spurious correlations. Saliency regularization has emerged as a promising way to achieve 
                this, but existing approaches typically require costly supervision such as human gaze data or manual 
                saliency annotations. In contrast, <span style="color: maroon; font-weight: bold">AutoFocus-IL</span> leverages vision-language models (VLMs) to automatically 
                identify and track key objects in demonstrations, generating temporal saliency maps that highlight 
                causal visual signals while suppressing distractors. These maps are then used to regularize behavior 
                cloning policies, yielding stronger alignment between visual attention and task-relevant cues. Our 
                findings highlight that VLM-driven saliency provides a scalable, annotation-free path toward robust 
                imitation learning in robotics. Particularly, our experiments in both the CARLA simulator and 
                real-robot manipulation tasks demonstrate that <span style="color: maroon; font-weight: bold">AutoFocus-IL</span> not only outperforms standard behavior 
                cloning but also surpasses state-of-the-art baselines that assume privileged access to human 
                supervision, such as gaze data.


            </div>
          </div>
        </div>
      </div>
    </section>

    <!-- Teaser Picture -->
    <section class="hero teaser" style="margin-bottom: 0px;">
      <div class="hero-body">
        <div class="container is-max-desktop" style="text-align: center;">
          <div class="columns is-centered has-text-centered">
            <div class="column is-four-fifths">          
          <div style="text-align: center;">
            <img src="./assets/files/Fig2.png" alt="method fig" style="width: 80%;">
          </div>
          Overview of the AutoFocus-IL pipeline.
          <br>
          We first use a VLM to identify and track task-relevant objects.
          Then, we generate temporal saliency maps that highlight these objects in each frame of the demonstration.
          Finally, we use these saliency maps to regularize the behavior cloning policy, encouraging it to focus on
          the highlighted regions while suppressing distractors.
          <hr>
        </div>
        </div>
        </div>
      </div>
    </section>

    <!-- Visualization section -->
    <h3 class="title is-2" style="text-align:center; margin-top: 85px;">Visualization of Tasks</h3>   
      <div class="container is-max-desktop" style="text-align: center;">
        <div class="content has-text-justified">
          <p>  This section contains visualizations of our experimental setups and 
            data processing steps. Both setups icludes original and confounded environments.
            A CARLA confounded environment, there are action-conditioned icons on top of each 
            frame as indicated in the upcoming videos. These overlays do not affect dynamics 
            or expert actions, but introduce spurious correlations intended to test robustness
            against causal confusion.
            <br>
            Moreover, in the real-world robot, we design a confounded variant by placing 
            task-irrelevant distractor objects in the background to induce visual causal 
            confusion. Although the scene contains many distractors, AutoFocus-IL attends 
            only to the most task-relevant objects as indicated in the videos below.  
            <br>
          </p>
            <h3 class="title is-3" style="text-align:center; margin-top: 45px;">Filtering via VLM</h3>                
          <p>
            In this part, we visualize how AutoFocus-IL utilizes the VLM to mimic the human eye 
            gaze by filtering non-relevant factors in the input step by step. First, the left most 
            column indicates the raw observation. Then the second column is the output of the VLM 
            when queried with the object detection. After that, there is the task-relevant objects 
            filtered by the VLM. Finally, the right most column is the overlaid saliency map.
          </p>
        </div>
        <br>
        Task: Lift Carrot (Confounded)
        <div class="gif-grid" id="gifGrid1">
          <p> Raw Observation</p>
          <p> Key Objects Detected</p>
          <p> Task-Relevant Objects</p>
          <p> Saliency Map</p>
        </div>

        <br>
        
        Task: Pull Pot (Confounded)
        <div class="gif-grid" id="gifGrid2">
          <p> Raw Observation</p>
          <p> Key Objects Detected</p>
          <p> Task-Relevant Objects</p>
          <p> Saliency Map</p>
        </div>

        <br>
        
        Task: Turn Left (Confounded)
        <div class="gif-grid" id="gifGrid3">
          <p> Raw Observation</p>
          <p> Key Objects Detected</p>
          <p> Task-Relevant Objects</p>
          <p> Saliency Map</p>
        </div>

        <br>

        Task: Change Lane (Confounded)
        <div class="gif-grid" id="gifGrid4">
          <p> Raw Observation</p>
          <p> Key Objects Detected</p>
          <p> Task-Relevant Objects</p>
          <p> Saliency Map</p>
        </div>

        <br>

        <h3 class="title is-3" style="text-align:center; margin-top: 45px;">Advantage of the trained model over BC</h3>                
        <div class="content has-text-justified">
          <p> In this part, we visualize how AutoFocus-IL performs a task successfully, while 
            BC fails due to the causal confusion. The left column is the BC model's performance, 
            the middle column indicates AutoFocus-IL's performance in different tasks,  and the 
            right column shows an example of performing the same task by a human.
          </p>
        </div>

        <br>

        Task: Lift the Carrot (Confounded)
        <br>
        <div class="gif-grid-comparison" id="gifGrid5">
          <p> Expert Demonstration </p>
          <p> BC </p>
          <p> AutoFocus-IL </p>
        </div>

        <br>

        Task: Lift the Carrot (Original)
        <br>
        <div class="gif-grid-comparison" id="gifGrid6">
          <p> Expert Demonstration </p>
          <p> BC </p>
          <p> AutoFocus-IL </p>
        </div>

        <br>

        Task: Pull the Pot (Confounded)
        <br>
        <div class="gif-grid-comparison" id="gifGrid7">
          <p> Expert Demonstration </p>
          <p> BC </p>
          <p> AutoFocus-IL </p>
        </div>

        <br>

        Task: Pull the Pot (Original)
        <br>
        <div class="gif-grid-comparison" id="gifGrid8">
          <p> Expert Demonstration </p>
          <p> BC </p>
          <p> AutoFocus-IL </p>
        </div>
        </div>

      <hr>

    </section>


    
    <!-- Section for displaying prompts to the VLM -->
    <section class="hero teaser" style="margin-bottom: 0px;"></section>
      <div class="hero-body">
        <div class="container is-max-desktop" style="text-align: center;">
          <div class="columns is-centered has-text-centered">
            <div class="column is-four-fifths">
               
              <h2 class="title is-2" style="text-align:center;">VLM Prompts</h2>
              <div class="content has-text-justified">
                <p> 
                  In this section, we provide example prompts used to generate saliency maps with the VLM. 
                  We used <code>Qwen2.5-VL-72B-Instruct</code> model to generate the saliency maps. 
                  The process of building saliency map consists of some consequent steps. In the following 
                  part, we first show the prompts in the CARLA environment, followed by the prompts in the 
                  real-world robot setup. 
                  <br>
                </p>
              </div>


              <h3 class="title is-3" style="text-align:center; margin-top: 85px;">CARLA</h3>   
              <div class="content has-text-justified">
                <p>
                  1. Trajectory Description Prompt of CARLA Environment:
                </p>
                <div class="prompt-container">
                  <div class="prompt-block collapsed" id="promptBlock"><strong>The following is a short video segment of an autonomous driving trajectory.
1. First, understand what the autonomous vehicle is doing in this video (e.g., approaching an intersection, stopping, turning, adjusting to other vehicles).

2. Identify and list all visible objects in the scene that are relevant to driving. For each object, provide:
  - type: the object’s category (e.g., Stop Sign, Speed Limit Sign, Lane Markings, Vehicles).
  - relevance: the role of this object in driving context (e.g., traffic_control, navigation, safety_critical, context).
  - description: one sentence describing its appearance, meaning, and role in driving.

3. Ignore irrelevant background objects or decorations that do not affect the driving task.

4. Ensure consistent naming style for object type (singular, capitalized, concise).

5. After listing objects, summarize the vehicle’s behavioral description in one sentence (e.g., “The vehicle approaches a stop sign, halts, then continues driving while maintaining lane alignment and adjusting speed to oncoming traffic.”).

6. Finally, output a metadata block summarizing:
  - total_objects: total count of unique driving-relevant objects (not including pure context objects like trees/buildings, unless explicitly relevant).
  - object_types: array of the key object categories detected.
  - processed_frames: number of video frames analyzed.

7. Output must strictly follow this JSON format and avoid redundant text:
<div class="json-block">json
    {
      "route": "&lt;route_id&gt;",
      "seed": "&lt;seed_id&gt;",
      "k_frames": &lt;number_of_frames&gt;,
      "objects": [
        {
          "type": "&lt;object_type&gt;",
          "relevance": "&lt;traffic_control|navigation|safety_critical|context&gt;",
          "description": "&lt;one-sentence description&gt;"
        }
      ],
      "description": "&lt;summary of what the vehicle is doing&gt;",
      "metadata": {
        "total_objects": &lt;int&gt;,
        "object_types": ["&lt;object1&gt;", "&lt;object2&gt;", "..."],
        "processed_frames": &lt;int&gt;
      }
    }</div></strong>
                </div>
                  <button class="toggle-btn" id="toggleBtn">Show Full Prompt</button>
                </div>
                  <p>
                    2. Filtering Stage Prompt
                  </p>
                <div class="prompt-container">
                  <div class="prompt-block collapsed" id="promptBlock"><strong>You are analyzing a scene for autonomous driving.
  Frame: {frame_id}
  Global Intent: {global_context}
  Action Intent: {action_context}
  Candidates (normalized xyxy):
  {candidates}
  Task:
  1) Select the top-K most relevant objects (<=3 by track_id) to safe driving and current intention.
  2) For same class but different track_id, pick the one with highest importance now.
  3) If you suspect missing categories for current frame, list them for re-detection.
  4) Return JSON with this exact schema:<div class="json-block">{
    "top_k": [
      {"id": &lt;int&gt;, "class": "&lt;str&gt;", "score": &lt;float 0~1&gt;, "rationale": "&lt;short&gt;"}
    ],
    "missing_suspects": ["&lt;class&gt;", ...]
  }</div>Rules:
- Prioritize collision risks (in-path vehicles, close pedestrians/cyclists), rule-governing items (traffic_light/sign), and trajectory-relevant actors.
- "score" is an importance weight; approximate is fine; we only use ids for filtering.
- Return JSON only (no extra text).</strong>
                </div>
                  <button class="toggle-btn" id="toggleBtn">Show Full Prompt</button>
                </div>

                </div>
              
              <h3 class="title is-3" style="text-align:center; margin-top: 45px;">Real-World Robot</h3>
                          <div class="content has-text-justified">
                <p>
                  1. Trajectory Description Prompt of WidowX Environment:
                </p>
                <div class="prompt-container">
                  <div class="prompt-block collapsed" id="promptBlock"><strong>Objective:
  The following is a video of a WidowX robotic arm operating trajectory.
  1. First, understand the robot's task in this video.
  2. Identify objects in the image that only interact with the robotic arm and output their descriptive keywords. Output must use the color + material + category format (for example: silver iron pot lid, blue plastic bowl). Do not include obstacles that the robotic arm may encounter during its trajectory.
  3. Ignore irrelevant decorations and distant or uninterpreted backgrounds.
  4. Ensure consistent naming style, using color + material + category throughout.
  5. Material and color must be clearly defined: for example, silver iron, black plastic, blue ceramic. Omit material or color if uncertain.
  6. The gripper/manipulator must *not* be in the output, for example: black robot gripper fingers, this should not be in the output.
  7. Components must be distinct: for example, a pot lid, cup handle, or knob should be output as pot lid, cup handle, stove knob, etc.
  8. Output must strictly follow the JSON format and avoid redundant text:
<div class="json-block">json
     {
      "objects": [
      {
      &quot;type&quot;: &quot;&lt;object_class or parent_part, e.g., microwave_handle, cup_handle, drawer_handle, stove_knob&gt;&quot;,
      &quot;state&quot;: &quot;&lt;open|closed|filled|empty|on|off|unknown&gt;&quot;,
      &quot;role&quot;: &quot;&lt;target|tool|container|support|obstacle|background&gt;&quot;,
      &quot;affordance&quot;: &quot;&lt;open|close|grasp|place|pour|press|pull|push|turn|none&gt;&quot;,
      }
      ],
      &quot;description&quot;: &quot;&lt;one-sentence action summary of what the robot/person is doing and key event&gt;&quot;
}</div></strong>
                </div>
                  <button class="toggle-btn" id="toggleBtn">Show Full Prompt</button>
                </div>
                  <p>
                    2. Filtering Stage Prompt
                  </p>
                <div class="prompt-container">
                  <div class="prompt-block collapsed" id="promptBlock"><strong>You are analyzing a household manipulation scene: {task_context}.
  Frame: {frame_id}
  Global Intent: {global_context}
  Action Intent: {action_context}
  Candidates (normalized xyxy):
  {candidates}
  Task:
  1) Based on the given bounding box and scene image, select the K objects most relevant to the robot successfully completing the task (sorted by track_id, including the robot gripper, the fewer the better, up to a maximum of three) to enable or block the current action.
  2) Prioritize direct targets (e.g., doors/handles/buttons/knobs), tools/containers, direct supports, and direct obstacles.
  3) If you believe there are potentially missing objects/parts in the current context, please list their classes for re-detection.
  4) Return JSON format:<div class="json-block">{
  "top_k": [
    {"id": <int>, "class": "<str>", "score": <float 0~1>, "rationale": "<short>"}
  ],
  "missing_suspects": ["<class>", ...]
}</div>Rules:
  - Explicitly address key components (e.g., microwave handles, cabinet doors, button panels).
  - Exclude distant/background items not relevant to the action. - "score" is the importance weight; an approximate value is sufficient; the ID is used for filtering.
  - Returns only JSON (no extra text).</strong>
                </div>
                  <button class="toggle-btn" id="toggleBtn">Show Full Prompt</button>
                </div>
              <br>
                
                <hr>

            </div>
          </div>
        </div>
      </div>
    </section>


    <!-- Results Secion -->
    <section class="hero teaser" style="margin-bottom: 0px;">
      <div class="hero-body">
        <div class="container is-max-desktop" style="text-align: center;">
          <div class="columns is-centered has-text-centered">
            <div class="column is-four-fifths">
              <h2 class="title is-2" style="text-align:center;">Results</h2>
              <h3 class="title is-3" style="text-align:center; margin-top: 45px;">CARLA</h3>                
              <div class="content has-text-justified">
              <p> 
                Evaluation is conducted on two disjoint sets: (1) seen
                routes, which are the same 10 routes used for training but
                with 2 different random seeds (20 evaluations per method),
                and (2) unseen routes, a disjoint set of 10 held-out routes,
                also with 2 seeds each (20 evaluations per method).
              </p>
              </div>
              <br>

              <div class="tab-item">
                <p> Original Environment</p>
                <br>
                <img src="./assets/files/CARLA_normal.png" alt="tabular" style="width: 100%;">
              <br> 
              <br> 
                <p style="margin-top: 20px;"> Confounded Environment</p>
                <br>
                <img src="./assets/files/CARLA_confounded.png" alt="tabular" style="width: 100%;">

              </a>

              </div>

              <h3 class="title is-3" style="text-align:center; margin-top: 45px;">Real-World Robot</h3>                
              <div class="content has-text-justified">
              <p> 
                For each of the four settings, we perform 10 rollouts and report the number of 
                successful episodes (task completion).We compare AutoFocus-IL against Behavior 
                Cloning (BC).
                <br>
                Results are summarized in the table below. AutoFocus-IL consistently improves 
                success on both tasks, with the largest gains under the confounded setting. 
                These trends mirror our simulation findings: VLM-driven, object-centric saliency 
                helps the policy focus on causal scene elements and markedly improves robustness 
                in the presence of unrelated visual clutter—without any additional human attention 
                labels. Note that prior baselines such as GABRIL are not applicable to this task, 
                as they require human gaze data—necessitating either specialized equipment for 
                gaze collection or complex projection methods to map gaze onto the robot’s 
                point-of-view camera.
              </p>
              </div>
              <br>

              <div class="tab-item">
                <p>Table of Real-Robot Results</p>
                <img src="./assets/files/real_robot_results.png" alt="tabular" style="width: 50%;">
              </div>

              <br>
              <hr>



    <script>
      document.querySelectorAll(".prompt-container").forEach(container => {
      const block = container.querySelector(".prompt-block");
      const btn = container.querySelector(".toggle-btn");

      btn.addEventListener("click", () => {
        block.classList.toggle("collapsed");
        btn.textContent = block.classList.contains("collapsed") ? "Show Full Prompt" : "Show less";
      });
    });

      // Generate the GIFs dynamically and add them to the grid
      window.onload = function() {
        const gifGrid1 = document.getElementById('gifGrid1');

        gifItem = document.createElement('div');
        gifImage = document.createElement('img');
        gifItem.classList.add('gif-item');

        gifImage.src = `./assets/files/bdv2_data_video/lift_carrot_100_confounded/original.gif`;

        gifItem.appendChild(gifImage);
        gifGrid1.appendChild(gifItem);

        gifItem = document.createElement('div');
        gifImage = document.createElement('img');
        gifItem.classList.add('gif-item');

        gifImage.src = `./assets/files/bdv2_data_video/lift_carrot_100_confounded/grounding_bbox.gif`;;

        gifItem.appendChild(gifImage);
        gifGrid1.appendChild(gifItem);

        gifItem = document.createElement('div');
        gifImage = document.createElement('img');
        gifItem.classList.add('gif-item');

        gifImage.src = `./assets/files/bdv2_data_video/lift_carrot_100_confounded/filtered_bbox.gif`;

        gifItem.appendChild(gifImage);
        gifGrid1.appendChild(gifItem);

        gifItem = document.createElement('div');
        gifImage = document.createElement('img');
        gifItem.classList.add('gif-item');

        gifImage.src = `./assets/files/bdv2_data_video/lift_carrot_100_confounded/heatmap_overlay.gif`;

        gifItem.appendChild(gifImage);
        gifGrid1.appendChild(gifItem);

        // ----------------------------------------------------------

        const gifGrid2 = document.getElementById('gifGrid2');

        gifItem = document.createElement('div');
        gifImage = document.createElement('img');
        gifItem.classList.add('gif-item');

        gifImage.src = `./assets/files/bdv2_data_video/pull_pot_100_confounded/original.gif`;

        gifItem.appendChild(gifImage);
        gifGrid2.appendChild(gifItem);

        gifItem = document.createElement('div');
        gifImage = document.createElement('img');
        gifItem.classList.add('gif-item');

        gifImage.src = `./assets/files/bdv2_data_video/pull_pot_100_confounded/grounding_bbox.gif`;;

        gifItem.appendChild(gifImage);
        gifGrid2.appendChild(gifItem);

        gifItem = document.createElement('div');
        gifImage = document.createElement('img');
        gifItem.classList.add('gif-item');

        gifImage.src = `./assets/files/bdv2_data_video/pull_pot_100_confounded/filtered_bbox.gif`;

        gifItem.appendChild(gifImage);
        gifGrid2.appendChild(gifItem);

        gifItem = document.createElement('div');
        gifImage = document.createElement('img');
        gifItem.classList.add('gif-item');

        gifImage.src = `./assets/files/bdv2_data_video/pull_pot_100_confounded/heatmap_overlay.gif`;

        gifItem.appendChild(gifImage);
        gifGrid2.appendChild(gifItem);

        // ----------------------------------------------------------

        const gifGrid3 = document.getElementById('gifGrid3');

        gifItem = document.createElement('div');
        gifImage = document.createElement('img');
        gifItem.classList.add('gif-item');

        gifImage.src = `./assets/files/carla_data_video/route_2416/seed_201/original.gif`;

        gifItem.appendChild(gifImage);
        gifGrid3.appendChild(gifItem);

        gifItem = document.createElement('div');
        gifImage = document.createElement('img');
        gifItem.classList.add('gif-item');

        gifImage.src = `./assets/files/carla_data_video/route_2416/seed_201/grounding_bbox.gif`;;

        gifItem.appendChild(gifImage);
        gifGrid3.appendChild(gifItem);

        gifItem = document.createElement('div');
        gifImage = document.createElement('img');
        gifItem.classList.add('gif-item');

        gifImage.src = `./assets/files/carla_data_video/route_2416/seed_201/filtered_bbox.gif`;

        gifItem.appendChild(gifImage);
        gifGrid3.appendChild(gifItem);

        gifItem = document.createElement('div');
        gifImage = document.createElement('img');
        gifItem.classList.add('gif-item');

        gifImage.src = `./assets/files/carla_data_video/route_2416/seed_201/heatmap_overlay.gif`;

        gifItem.appendChild(gifImage);
        gifGrid3.appendChild(gifItem);

        // ----------------------------------------------------------

        const gifGrid4 = document.getElementById('gifGrid4');

        gifItem = document.createElement('div');
        gifImage = document.createElement('img');
        gifItem.classList.add('gif-item');

        gifImage.src = `./assets/files/carla_data_video/route_26408/seed_201/original.gif`;

        gifItem.appendChild(gifImage);
        gifGrid4.appendChild(gifItem);

        gifItem = document.createElement('div');
        gifImage = document.createElement('img');
        gifItem.classList.add('gif-item');

        gifImage.src = `./assets/files/carla_data_video/route_26408/seed_201/grounding_bbox.gif`;;

        gifItem.appendChild(gifImage);
        gifGrid4.appendChild(gifItem);

        gifItem = document.createElement('div');
        gifImage = document.createElement('img');
        gifItem.classList.add('gif-item');

        gifImage.src = `./assets/files/carla_data_video/route_26408/seed_201/filtered_bbox.gif`;

        gifItem.appendChild(gifImage);
        gifGrid4.appendChild(gifItem);

        gifItem = document.createElement('div');
        gifImage = document.createElement('img');
        gifItem.classList.add('gif-item');

        gifImage.src = `./assets/files/carla_data_video/route_26408/seed_201/heatmap_overlay.gif`;

        gifItem.appendChild(gifImage);
        gifGrid4.appendChild(gifItem);

        // ----------------------------------------------------------

        const gifGrid5 = document.getElementById('gifGrid5');

        gifItem = document.createElement('div');
        gifImage = document.createElement('img');
        gifItem.classList.add('gif-item');

        gifImage.src = `./assets/files/real_robot_comparison/lift_carrot/confounded/lift_carrot_human_confounded.gif`;
        
        gifItem.appendChild(gifImage);
        gifGrid5.appendChild(gifItem);
        
        gifItem = document.createElement('div');
        gifImage = document.createElement('img');
        gifItem.classList.add('gif-item');
        
        gifImage.src = `./assets/files/real_robot_comparison/lift_carrot/confounded/lift_carrot_bc_confounded.gif`;
        
        gifItem.appendChild(gifImage);
        gifGrid5.appendChild(gifItem);

        gifItem = document.createElement('div');
        gifImage = document.createElement('img');
        gifItem.classList.add('gif-item');
        
        gifImage.src = `./assets/files/real_robot_comparison/lift_carrot/confounded/lift_carrot_autofocusil_confounded.gif`;;

        gifItem.appendChild(gifImage);
        gifGrid5.appendChild(gifItem);

        gifItem = document.createElement('div');
        gifImage = document.createElement('img');
        gifItem.classList.add('gif-item');

        // ----------------------------------------------------------

        const gifGrid6 = document.getElementById('gifGrid6');

        gifItem = document.createElement('div');
        gifImage = document.createElement('img');
        gifItem.classList.add('gif-item');

        gifImage.src = `./assets/files/real_robot_comparison/lift_carrot/original/lift_carrot_human_original.gif`;
        
        gifItem.appendChild(gifImage);
        gifGrid6.appendChild(gifItem);
        
        gifItem = document.createElement('div');
        gifImage = document.createElement('img');
        gifItem.classList.add('gif-item');
        
        gifImage.src = `./assets/files/real_robot_comparison/lift_carrot/original/lift_carrot_bc_original.gif`;
        
        gifItem.appendChild(gifImage);
        gifGrid6.appendChild(gifItem);
        
        gifItem = document.createElement('div');
        gifImage = document.createElement('img');
        gifItem.classList.add('gif-item');
        
        gifImage.src = `./assets/files/real_robot_comparison/lift_carrot/original/lift_carrot_autofocusil_original.gif`;;

        gifItem.appendChild(gifImage);
        gifGrid6.appendChild(gifItem);

        gifItem = document.createElement('div');
        gifImage = document.createElement('img');
        gifItem.classList.add('gif-item');

        // ----------------------------------------------------------

        const gifGrid7 = document.getElementById('gifGrid7');

        gifItem = document.createElement('div');
        gifImage = document.createElement('img');
        gifItem.classList.add('gif-item');

        gifImage.src = `./assets/files/real_robot_comparison/pull_pot/confounded/pull_pot_human_confounded.gif`;

        gifItem.appendChild(gifImage);
        gifGrid7.appendChild(gifItem);

        gifItem = document.createElement('div');
        gifImage = document.createElement('img');
        gifItem.classList.add('gif-item');

        gifImage.src = `./assets/files/real_robot_comparison/pull_pot/confounded/pull_pot_bc_confounded.gif`;

        gifItem.appendChild(gifImage);
        gifGrid7.appendChild(gifItem);

        gifItem = document.createElement('div');
        gifImage = document.createElement('img');
        gifItem.classList.add('gif-item');

        gifImage.src = `./assets/files/real_robot_comparison/pull_pot/confounded/pull_pot_autofocusil_confounded.gif`;;

        gifItem.appendChild(gifImage);
        gifGrid7.appendChild(gifItem);

        gifItem = document.createElement('div');
        gifImage = document.createElement('img');
        gifItem.classList.add('gif-item');

        // ----------------------------------------------------------

        const gifGrid8 = document.getElementById('gifGrid8');

        gifItem = document.createElement('div');
        gifImage = document.createElement('img');
        gifItem.classList.add('gif-item');

        gifImage.src = `./assets/files/real_robot_comparison/pull_pot/original/pull_pot_human_original.gif`;

        gifItem.appendChild(gifImage);
        gifGrid8.appendChild(gifItem);

        gifItem = document.createElement('div');
        gifImage = document.createElement('img');
        gifItem.classList.add('gif-item');

        gifImage.src = `./assets/files/real_robot_comparison/pull_pot/original/pull_pot_bc_original.gif`;

        gifItem.appendChild(gifImage);
        gifGrid8.appendChild(gifItem);

        gifItem = document.createElement('div');
        gifImage = document.createElement('img');
        gifItem.classList.add('gif-item');

        gifImage.src = `./assets/files/real_robot_comparison/pull_pot/original/pull_pot_autofocusil_original.gif`;;

        gifItem.appendChild(gifImage);
        gifGrid8.appendChild(gifItem);

        gifItem = document.createElement('div');
        gifImage = document.createElement('img');
        gifItem.classList.add('gif-item');

      };
    </script>

    <!-- <section class="section" id="BibTeX">
      <div class="container is-max-desktop content">
        <h2 class="title">BibTeX</h2>
        <pre><code>
          @inproceedings{author1Name2025autofocusil,
            title={AutoFocus-IL: VLM-based Saliency Maps for Data-Efficient Visual Imitation Learning without Extra Human Annotations},
            author={Author Surname, Author Given Name},
            booktitle={2026 IEEE International Conference on Robotics and Automation (ICRA)},
            year={2026},
            organization={IEEE}
          }          
        </code></pre>
      </div>
    </section> -->

    <footer class="footer">
      <div class="container">
        <div class="content has-text-centered">
          <!-- <a class="icon-link" href="./static/videos/nerfies_paper.pdf">
            <svg class="svg-inline--fa fa-file-pdf fa-w-12" aria-hidden="true" focusable="false" data-prefix="fas" data-icon="file-pdf" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 384 512" data-fa-i2svg=""><path fill="currentColor" d="M181.9 256.1c-5-16-4.9-46.9-2-46.9 8.4 0 7.6 36.9 2 46.9zm-1.7 47.2c-7.7 20.2-17.3 43.3-28.4 62.7 18.3-7 39-17.2 62.9-21.9-12.7-9.6-24.9-23.4-34.5-40.8zM86.1 428.1c0 .8 13.2-5.4 34.9-40.2-6.7 6.3-29.1 24.5-34.9 40.2zM248 160h136v328c0 13.3-10.7 24-24 24H24c-13.3 0-24-10.7-24-24V24C0 10.7 10.7 0 24 0h200v136c0 13.2 10.8 24 24 24zm-8 171.8c-20-12.2-33.3-29-42.7-53.8 4.5-18.5 11.6-46.6 6.2-64.2-4.7-29.4-42.4-26.5-47.8-6.8-5 18.3-.4 44.1 8.1 77-11.6 27.6-28.7 64.6-40.8 85.8-.1 0-.1.1-.2.1-27.1 13.9-73.6 44.5-54.5 68 5.6 6.9 16 10 21.5 10 17.9 0 35.7-18 61.1-61.8 25.8-8.5 54.1-19.1 79-23.2 21.7 11.8 47.1 19.5 64 19.5 29.2 0 31.2-32 19.7-43.4-13.9-13.6-54.3-9.7-73.6-7.2zM377 105L279 7c-4.5-4.5-10.6-7-17-7h-6v128h128v-6.1c0-6.3-2.5-12.4-7-16.9zm-74.1 255.3c4.1-2.7-2.5-11.9-42.8-9 37.1 15.8 42.8 9 42.8 9z"></path></svg>
          </a> -->
          <a class="icon-link" href="https://autofocus-il.github.io/" disabled="">
            <svg class="svg-inline--fa fa-github fa-w-16" aria-hidden="true" focusable="false" data-prefix="fab" data-icon="github" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512" data-fa-i2svg=""><path fill="currentColor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg><!-- <i class="fab fa-github"></i> Font Awesome fontawesome.com -->
          </a>
        </div>
        <div class="columns is-centered">
          <div class="column is-8">
            <div class="content">
              <p>
                This website is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
                  Commons Attribution-ShareAlike 4.0 International License</a>.
              </p>
              <p>
                Website design borrowed from <a href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>
              </p>
            </div>
          </div>
        </div>
      </div>
    </footer>

  

</body></html>